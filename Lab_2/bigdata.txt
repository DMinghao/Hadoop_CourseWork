Internet of Things (IoT) is connecting almost all the components together in every aspect of business and our daily life. As a result, huge amount of data is being generated. The term “big data” implies the large scale of data that cannot be stored on one single computer. The analyses of such large-scaled data usually require massively parallel software running on tens, hundreds, or even thousands of servers. Enterprise technology managers are often called upon to organize large-scaled data repositories, to manage and schedule resources between technology components, and to support decision making based on information that resides in distributed data sources. This course prepares students with fundamental concepts of distributed data systems and analytics algorithms. It equips students with advanced techniques to extract the value from the large-scaled data generated and collected in everyday business life. The course uses a hands-on, learning-by-doing approach to understand some of the key technologies within the Hadoop ecosystem, which is the current state of art to provide a framework for distributed storage and processing of large-scaled data. Topics include: enterprise Application Programing Interfaces (APIs), API connectivity to distributed networks, MapReduce model, distributed file system (HDFS), distributed system resources scheduling (Yarn) and user interface (Hue), transferring data in and out of Hadoop (Sqoop), distributed data warehousing (Hive), and high-level distributed platforms such as Pig and Spark. The focus is on creating awareness of the technologies, allowing some level of familiarity with them through assignments, and enabling some strategic thinking around the use of these in business. 