{
 "cells": [
  {
   "source": [
    "MapReduce vs. Spark (https://www.xplenty.com/blog/apache-spark-vs-hadoop-mapreduce/)\n",
    "\n",
    "Why do we always repeat work_count example?\n",
    "\n",
    "The first reason is because it is the easiest one example to understand the logic behind hadoop for rookies in this area lol. \n",
    "The second reason is that this example has huge amount of applications.\n",
    "The most straightforward example is that it helps companies like Amazon to calculate which items are the most frequently bought ones by customers. \n",
    "We can also figure out the most frequent doubleton or tripleton items. I believe everyone heard of the beer and diaper case, right? That is an easy task for Hadoop system.\n",
    "\n",
    "If you really want to dig deeper, MapReduce/Spark also can do a lot of amazing things, like Matrix Multiplication. (example http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/9-parallel/matrix-mult.html)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os \n",
    "\n",
    "os.environ['HADOOP_HOME'] = os.path.abspath(os.path.join(os.getcwd(), os.pardir)).replace('\\\\', '/')+'/util/hadoop-2.7.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "input.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = None \n",
    "# count = 0\n",
    "while sc is None: \n",
    "    try: \n",
    "        sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local\"))\n",
    "    except: \n",
    "        pass\n",
    "        # print(f\"tried {count} time\", end='\\r')\n",
    "        # count += 1\n",
    "\n",
    "# Starting off by reading the input file as text_file\n",
    "text_file = sc.textFile(\"input.txt\")\n",
    "text_file"
   ]
  },
  {
   "source": [
    "## explanation: \n",
    "### step1:\n",
    "```bash\n",
    "flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "we need to break the file into lines for which splitting on the base of “ ” is required.\n",
    "\n",
    "Here **lambda** function is an anonymous function. Sometimes we use anonymous function to make the code tighter.\n",
    "\n",
    "It is exactly the same as:\n",
    "```\n",
    "def split_word(line):\n",
    "    return line.split(\" \")\n",
    "    \n",
    "flatMap(split_word)\n",
    "```\n",
    "### step2:\n",
    "```\n",
    "map(lambda word: (word, 1))\n",
    "```\n",
    "map function has the same use like the MapReduce **Mapper** function. \n",
    "\n",
    "It counts each word 1 time every time it appears. \n",
    "```\n",
    "Like [I I love hadoop] --> (I,1) (I,1) (love,1) (hadoop,1)\n",
    "```\n",
    "Remember: It won't do things like (I,2) because we define **lambda word: (word, 1)** in the map function\n",
    "\n",
    "### Step3:\n",
    "```\n",
    "reduceByKey(lambda a, b: a + b)\n",
    "```\n",
    "Since this function works by **key**, a and b here are the values of previous step.\n",
    "\n",
    "For example, \n",
    "```\n",
    "input for this step: (I,1) (I,1) (love,1) (hadoop,1)\n",
    "output for this step: (I,2) (love,1) (hadoop,1)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = text_file.flatMap(lambda line: line.split(\" \"))\n",
    "counts = counts.map(lambda word: (word, 1))\n",
    "counts = counts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "shutil.rmtree('output', ignore_errors=True)\n",
    "\n",
    "### output file must not exist!!! Otherwise, it will appear error.\n",
    "counts.saveAsTextFile(\"./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here is the same map reduce scripts without any lambda function\n",
    "'''\n",
    "\n",
    "def split_word(line):\n",
    "    return line.split(\" \")\n",
    "\n",
    "def word_ct(word):\n",
    "    return (word,1)\n",
    "\n",
    "def value_add(a,b):\n",
    "    return a+b\n",
    "\n",
    "counts = text_file.flatMap(split_word) \\\n",
    "             .map(word_ct) \\\n",
    "             .reduceByKey(value_add)\n",
    "             \n",
    "shutil.rmtree('output1', ignore_errors=True)\n",
    "\n",
    "### output file must not exist!!! Otherwise, it will appear error.\n",
    "counts.saveAsTextFile(\"output1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c627be1547c7c874688ddee1aaaa67b367275f1752d282bff5eb427acd241334"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}